{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4e5ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igorg\\Documents\\Programação\\PDSI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "from transformers import CLIPTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b453e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_PATH = \"C:\\\\Users\\\\igorg\\\\Documents\\\\Programação\\\\PDSI\\\\imagens\\\\Imagem1.webp\"\n",
    "\n",
    "LORA_FOLDER = \"C:\\\\Users\\\\igorg\\\\Documents\\\\Programação\\\\PDSI\\\\output\\\\pintura_oleo_style\"\n",
    "\n",
    "BASE_MODEL_ID = \"stablediffusionapi/anything-v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14baba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGGER_WORD = \"pintura_oleo_art\"  # ou a palavra certa do seu LoRA\n",
    "\n",
    "PROMPT = f\"{TRIGGER_WORD}, tree, nature, landscape, outdoors, oil painting, heavy impasto, brush strokes, textured canvas, traditional art, masterpiece, rich colors\"\n",
    "\n",
    "# Remova coisas de humano do negativo para garantir, mas deixe o básico\n",
    "NEGATIVE_PROMPT = \"photo, realistic, 3d render, smooth, digital art, vector, low quality, bad anatomy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4989d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./resultados_style_PO\"\n",
    "SEED = 42\n",
    "NUM_STEPS = 30\n",
    "GUIDANCE_SCALE = 7.5\n",
    "STRENGTH = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6728f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando com GPU: NVIDIA GeForce RTX 3050 Laptop GPU ---\n",
      "Carregando modelo base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at stablediffusionapi/anything-v5 were not used when initializing CLIPTextModel: ['text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\igorg\\Documents\\Programação\\PDSI\\venv\\Lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.01it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecionados 5 checkpoints finais para teste.\n",
      "Encontrados 5 checkpoints.\n",
      "--> Testando: pintura_oleo_style-06.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from state_dict led to unexpected keys not found in the model: text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight. \n",
      "100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: teste_pintura_oleo_style-06.png\n",
      "--> Testando: pintura_oleo_style-07.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from state_dict led to unexpected keys not found in the model: text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight. \n",
      "100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: teste_pintura_oleo_style-07.png\n",
      "--> Testando: pintura_oleo_style-08.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from state_dict led to unexpected keys not found in the model: text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight. \n",
      "100%|██████████| 18/18 [00:08<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: teste_pintura_oleo_style-08.png\n",
      "--> Testando: pintura_oleo_style-09.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from state_dict led to unexpected keys not found in the model: text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight. \n",
      "100%|██████████| 18/18 [00:08<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: teste_pintura_oleo_style-09.png\n",
      "--> Testando: pintura_oleo_style-10.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from state_dict led to unexpected keys not found in the model: text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight, text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight, text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight. \n",
      "100%|██████████| 18/18 [00:08<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: teste_pintura_oleo_style-10.png\n",
      "\n",
      "SUCESSO! Verifique a pasta 'resultados_finais'.\n"
     ]
    }
   ],
   "source": [
    "def run_style_transfer():\n",
    "    print(f\"--- Iniciando com GPU: {torch.cuda.get_device_name(0)} ---\")\n",
    "    \n",
    "    # 1. Carregar Modelo Base\n",
    "    print(f\"Carregando modelo base...\")\n",
    "    \n",
    "    # Clip Skip 2\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        BASE_MODEL_ID, \n",
    "        subfolder=\"text_encoder\", \n",
    "        num_hidden_layers=11\n",
    "    )\n",
    "\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        text_encoder=text_encoder,\n",
    "        torch_dtype=torch.float16, \n",
    "        safety_checker=None\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Otimizações de memória para a RTX 3050\n",
    "    pipe.enable_attention_slicing()\n",
    "    pipe.enable_model_cpu_offload() # <--- ESSA É A MÁGICA PARA 4GB/6GB VRAM\n",
    "\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    # 2. Carregar Imagem\n",
    "    if not os.path.exists(INPUT_IMAGE_PATH):\n",
    "        print(f\"ERRO: A imagem '{INPUT_IMAGE_PATH}' não existe na pasta!\")\n",
    "        return\n",
    "\n",
    "    init_image = load_image(INPUT_IMAGE_PATH).resize((512, 512))\n",
    "\n",
    "    lora_files = sorted([f for f in os.listdir(LORA_FOLDER) if f.endswith(\".safetensors\")])\n",
    "\n",
    "    lora_files = lora_files[-5:]\n",
    "\n",
    "    print(f\"Selecionados {len(lora_files)} checkpoints finais para teste.\")\n",
    "\n",
    "    print(f\"Encontrados {len(lora_files)} checkpoints.\")\n",
    "\n",
    "    for lora_file in lora_files:\n",
    "        lora_path = os.path.join(LORA_FOLDER, lora_file)\n",
    "        print(f\"--> Testando: {lora_file}\")\n",
    "\n",
    "        try:\n",
    "            pipe.load_lora_weights(lora_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar LoRA: {e}\")\n",
    "            continue\n",
    "\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(SEED)\n",
    "        \n",
    "        # --- A CORREÇÃO ESTÁ AQUI: AUTOCAST ---\n",
    "        # Isso converte automaticamente os tipos float32/float16 na hora certa\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            result = pipe(\n",
    "                prompt=PROMPT,\n",
    "                image=init_image,\n",
    "                strength=STRENGTH,\n",
    "                negative_prompt=NEGATIVE_PROMPT,\n",
    "                num_inference_steps=28,\n",
    "                guidance_scale=GUIDANCE_SCALE,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        # --------------------------------------\n",
    "\n",
    "        save_name = f\"teste_{lora_file.replace('.safetensors', '')}.png\"\n",
    "        result.save(os.path.join(OUTPUT_FOLDER, save_name))\n",
    "        print(f\"Salvo: {save_name}\")\n",
    "\n",
    "        pipe.unload_lora_weights()\n",
    "\n",
    "    print(\"\\nSUCESSO! Verifique a pasta 'resultados_finais'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_style_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed90ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
